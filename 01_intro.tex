\section{Introduction}

In the last few years, multimedia data collection has shown a noticeable increase.
Large-scale multimedia data collection requires storing and indexing massive databases.
Annotating this data is a tiresome task and can hardly be conducted manually.
Hence, the need for automatic approaches is growing.
The prevailing methodologies in the present context are primarily centered around neural networks.
However, the relationship between the input data and the decision is mostly untraceable.
The lack of knowledge in the decision-making process can be an issue in some contexts (e.g. forensic, broadcast news automatic labeling...). 
This paper focuses on automatic audio segmentation, i.e. the detection of homogeneous segments containing speech (SAD), music (MD), noise (ND), and overlapped speech (OSD) with a unified model.
This task is initially performed by a black-box neural model. 
We propose a proxy explainable model which is trained to fit the original model distribution while showing noticeable explanation capacities. 

%Historically, audio signal segmentation is performed using statistical models.
Early studies on speech \cite{sohn1999statistical}, overlapped speech \cite{charlet_impact_2013}, and music \cite{lavner2009decision} segmentation are focused on the statistical modeling of manually extracted acoustic features.
Currently, the segmentation is performed with neural networks and supervised learning.
Most approaches consist of the classification of acoustic features at the frame level.
Each task was first solved separately as binary frame classification tasks.
SAD \cite{lavechin2019end}, OSD \cite{bullock_overlap-aware_2020,lebourdais22_interspeech}, MD \cite{jang2019music,de2019exploring}.
Several approaches have also been proposed to solve multiple tasks simultaneously.
For example, in \cite{gimeno2020multiclass}, authors propose to segment speech, music, and noise with a single multiclass model.
A few works also report joint SAD and OSD \cite{jung21_interspeech,bredin21_interspeech,lebourdais2023joint}.
In this paper, SAD, OSD, MD, and ND are simultaneously solved as a multilabel frame-classification task.

The proposed approach aims to train an explainable proxy model from the black-box pre-trained segmentation system.
While many works have been conducted in explaining neural networks \cite{}, the literature is limited in the audio domain.
The first intents are focused on saliency map extraction to reveal what information is used in the output \cite{}.
Recently, a few explainable models have been developed like APNet \cite{zinemanas2021interpretable}, which extends the prototypical network to the audio domain.
The proposed architecture is mainly inspired by \cite{parekh2023tackling}.
The authors propose to explain an audio classification model by a proxy model which is optimized to classify audio scenes while reconstructing the audio with the non-negative matrix factorization framework \cite{lee2000algorithms}.
This model can be used both as post-hoc and by design explainability.

In this paper, we propose to train an explainable proxy model from a pre-trained multilabel segmentation model (designated as the teacher).
This model inputs Wavlm pre-trained features and outputs the pseudo probability of each class.
Two types of proxy models are investigated.
The former inputs a spectrogram as features and the latter uses the teacher's Wavlm outputs.
The proxy models are inspired by \cite{parekh2023tackling}.
We demonstrate that proxy models provide similar or even better performance as the original model while being smaller and explainable.
We show that the relevant information for classification can be mapped to the spectral domain.
The relevance is confirmed by a confidence measure.
To the best of our knowledge, this is the first intent to design an explainable neural segmentation model.  

This paper is arranged as follows: sect. \ref{sect:models} presents the structure of the system along with the proxy model training strategy.
Sect. \ref{sect:protocol} introduces the experimental protocol and sect. \ref{sect:perf} the segmentation results. 
Sect. \ref{sect:explain} demonstrates the explainability capacities of the proposed system.









